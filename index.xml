<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Betterfor - A dreamful developer</title>
    <link>https://betterfor.github.io/</link>
    <description>Recent content on Betterfor - A dreamful developer</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 17 Jul 2019 19:51:14 +0800</lastBuildDate>
    
        <atom:link href="https://betterfor.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>K8s概念详解(二)</title>
      <link>https://betterfor.github.io/post/k8s2/</link>
      <pubDate>Wed, 17 Jul 2019 19:51:14 +0800</pubDate>
      
      <guid>https://betterfor.github.io/post/k8s2/</guid>
      
        <description>

&lt;h3 id=&#34;四-通过service访问pod&#34;&gt;四、通过Service访问Pod&lt;/h3&gt;

&lt;h5 id=&#34;4-1-创建service&#34;&gt;4.1、创建Service&lt;/h5&gt;

&lt;p&gt;kubernetes Service从逻辑上代表了一组Pod，具体是哪些pod是由label来挑选的。Service有自己的IP，而这个IP不变。客户端只需要访问Service的IP。kubernetes则负责建立和维护Service与Pod的映射关系。无论后端pod如何变化，对客户端不会有任何影响，因为service没变。&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/service_labels.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/service_labels.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;




&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/service_ip.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/service_ip.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;pod分配了各自的IP，这些IP只能被kubernetes Cluster中的容器和节点访问。&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/curl_ip.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/curl_ip.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;




&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/service_apiVersion.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/service_apiVersion.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;v1是Service的apiVersion&lt;/li&gt;
&lt;li&gt;指明当前的资源为Service&lt;/li&gt;
&lt;li&gt;Service的名称为httpd-svc&lt;/li&gt;
&lt;li&gt;selector指明挑选哪些label为run：httpd的pod作为Service的后端&lt;/li&gt;
&lt;li&gt;将Service的8080端口映射到pod的80端口&lt;/li&gt;
&lt;/ul&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/svc.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/svc.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;httpd-svc分配到一个cluster-ip 10.106.65.48。可以通过该ip访问后端的httpd pod。根据前面的端口映射，这里要使用8080端口。另外，除了我们创建的httpd-svc，还有一个Service kubernetes，cluster内部通过这个Service访问kubernetes API Server。&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/httpd_svc.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/httpd_svc.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;endpoint罗列了3个pod的ip和端口，我们知道pod的ip是在容器中配置的，那么service的cluster ip是在哪配置的？cluster ip是怎么映射到pod IP的？是iptables。&lt;/p&gt;

&lt;h5 id=&#34;4-2-cluster-ip底层实现&#34;&gt;4.2、cluster IP底层实现&lt;/h5&gt;

&lt;p&gt;cluster IP是一个虚拟IP，由kubernetes 节点上的iptables规则管理。可以通过iptables-save命令答应当前节点的iptables规则&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/iptable1.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/iptable1.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;如果cluster内的pod（原地址来源10.244.0.0/16）要访问httpd-svc，则允许。&lt;/li&gt;
&lt;li&gt;其他源地址访问httpd-svc，跳转到规则KUBE-SVC-RL3JAE4GN7VOGDGP&lt;/li&gt;
&lt;/ul&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/iptable2.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/iptable2.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;各有1/3概率跳转到规则（后三条）&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/iptable3.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/iptable3.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;即将请求分别转发到后端的三个pod。通过上面的分析，我们得到结论：iptables将访问service的流量转发到后端pod，而且使用类似于轮询的负载均衡策略。cluster的每个节点都配置了相同的iptables规则，这样就确保整个cluster都能够通过service的cluster IP访问service。&lt;/p&gt;

&lt;h5 id=&#34;4-3-dns访问service&#34;&gt;4.3、DNS访问Service&lt;/h5&gt;

&lt;p&gt;在cluster中，除了可以通过cluster IP访问Service，kubernetes还提供更方便的DNS访问。kubeadm部署会默认安装kube-dns组件（没安装）。kube-dns是一个DNS服务器。每当有新的Service被创建，kube-dns会添加Service的DNS记录。&lt;/p&gt;

&lt;p&gt;cluster中的pod可以通过&lt;SERVICE_NAME&gt;.&lt;NAMESPACE_NAME&gt;访问Service。比如可以用httpd-svc.default访问Service httpd-svc&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/dns_svc.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/dns_svc.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;h5 id=&#34;4-4-外网访问service&#34;&gt;4.4、外网访问service&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;clusterIP：service通过cluster内部的ip对外提供服务，只有cluster内的节点和pod可访问，这是默认的service类型。&lt;/li&gt;
&lt;li&gt;NodePort：service通过cluster节点的静态端口对外提供服务。cluster外部可通过&lt;NodeIP&gt;:&lt;NodePort&gt;访问service。&lt;/li&gt;
&lt;li&gt;LoadBalancer：service利用cloud provider特有的load balancer对外提供服务，cloud provider负责将load balancer的流量导向service。目前支持的cloud provider有GCP\AWS\AZUR等&lt;/li&gt;
&lt;/ul&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/nodeport.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/nodeport.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;添加type：NodePort&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/nodeport_svc.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/nodeport_svc.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;kubernetes依然会为httpd-svc分配一个clusterIP，不同的是&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;EXTERNAL-IP为nodes，表示可通过cluster每个节点自身的ip访问service。&lt;/li&gt;
&lt;li&gt;port（s）为8080:31347.8080是cluster监听端口，31347则是节点上监听的端口。kubernetes会从30000-32767中分配一个可用的端口，每个节点都会监听此端口并将请求转发给service&lt;/li&gt;
&lt;/ul&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/port.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/port.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;与clusterIP一样，借助iptables。与clusterIP相比，每个节点的iptables中都增加规则&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/default_svc.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/default_svc.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;作用是负载均衡到每个pod。NodePort默认随机选择，可以用nodePort指定特定端口&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/nodeports.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/nodeports.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;现在配置文件有三个port&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;nodePort是节点上的监听端口&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;port是ClusterIP上监听端口&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;targetPort是Pod监听端口&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最终，Node和ClusterIP在各自端口上接收到的请求都会通过iptables转发到Pod的targetPort。&lt;/p&gt;

&lt;h3 id=&#34;五-rolling-update&#34;&gt;五、Rolling Update&lt;/h3&gt;

&lt;h5 id=&#34;5-1-升级&#34;&gt;5.1、升级&lt;/h5&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/httpd_deploy31.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/httpd_deploy31.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;将配置文件中的镜像httpd:2.2.32&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/httpd_deploy32.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/httpd_deploy32.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;deployment httpd镜像更新为httpd：2.2.32&lt;/li&gt;
&lt;li&gt;新创建replicaset httpd-8bdffc6d8，镜像为httpd：2.2.32，并且管理三个新的pod&lt;/li&gt;
&lt;li&gt;之前的httpd-5ddb558f47里没有任何pod&lt;/li&gt;
&lt;li&gt;结论：replicaset httpd-5ddb558f47的三个httpd：2.2.31 pod被replicaset httpd-8bdffc6d8的三个httpd：2.2.32 Pod替换&lt;/li&gt;
&lt;/ul&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/httpd_deploy_describe.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/httpd_deploy_describe.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;每次更新只替换一个pod。每次替换的pod数量是可定制的，kubernetes提供了两个参数maxSurge和maxUnavailable来精细控制pod的替换个数。&lt;/p&gt;

&lt;h5 id=&#34;5-2-回滚&#34;&gt;5.2、回滚&lt;/h5&gt;

&lt;p&gt;kubectl apply每次更新应用时，kubernetes都会记录当前的配置，保存为一个revision（版次），这样就可以回退到某个特定的revision。默认配置下，kubernetes只会保留最近几个revision，可以在deployment配置文件中通过revisionHistoryLimit属性增加revision数量&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/httpd17.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/httpd17.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;




&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/httpd18.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/httpd18.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;




&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/httpd19.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/httpd19.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;




&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/apply_httpd.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/apply_httpd.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;&amp;ndash;record的作用是将当前命令记录到revision记录中，这样就可以知道每个revision对应的是哪个配置文件。&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/revision.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/revision.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;CHANGE-CAUSE就是&amp;ndash;record的结果。如果要回退到某个版本，比如revision 3&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/undo.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/undo.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;此时，revision历史记录也会发生相应的变化。所以执行kubectl apply时加上&amp;ndash;record参数&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/rollout.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/rollout.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;h3 id=&#34;六-health-check&#34;&gt;六、Health Check&lt;/h3&gt;

&lt;p&gt;强大的自愈能力是kubernetes这类容器编排引擎的一个重要特性。自愈的默认实现方式是自动重启发生故障的容器。初次之外，用户还可以利用Liveness和Readiness探测机制设置更精细的健康检查，进而实现如下需求：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;零停机部署&lt;/li&gt;
&lt;li&gt;避免故障无效的镜像&lt;/li&gt;
&lt;li&gt;更加安全的滚动升级&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&#34;6-1-默认的健康检查&#34;&gt;6.1、默认的健康检查&lt;/h5&gt;

&lt;p&gt;每个容器启动时会执行一个进程，此进程由Dockerfile的CMD或EntryPoint指定。如果进程退出时返回码非0，则认为容器发生故障，kubernetes就会根据restartPolicy重启容器。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  labels:
    test: healthcheck
  name: healthcheck
spec:
  restartPolicy: OnFailure
  containers:
  - name: healthcheck
    image: busybox
    args:
    - /bin/sh
    - -c
    - sleep 10;exit 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod的restartPolicy设置为OnFailure，默认为Alawys。sleep 10;exit 1模拟容器启动10秒后发生故障&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/healthCheck.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/healthCheck.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;可看到容器当前已经重启了2次。在上面的例子中，容器进程返回值非0，kubernetes则认为容器发生故障，需要重启。有不少情况是发生了故障，但容器并不会退出。比如访问web服务器时显示500内部错误，可能是系统超载，也可能是资源死锁，此时httpd进程并没有异常退出，在这种情况下重启容器可能是最直接、最有效的解决方案。&lt;/p&gt;

&lt;h5 id=&#34;6-2-linveness探测&#34;&gt;6.2、Linveness探测&lt;/h5&gt;

&lt;p&gt;Liveness探测让用户可以自定义判断容器是否健康的条件。如果探测失败，kubernetes就会重启容器。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness
spec:
  restartPolicy: OnFailure
  containers:
  - name: liveness
    image: busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy;sleep 30;rm -rf /tmp/healthy;sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 10
      periodSeconds: 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动进程首先创建文件 /tmp/healthy，30秒后删除，在我们设定中，如果/tmp/healthy文件存在，则认为容器处于正常状态，反之则发生故障。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;探测方法：通过cat检查/tmp/healthy文件是否存在。如果命令执行成功，返回值为0，kubernetes则认为本次Liveness探测成功；如果命令返回值非0，本次Liveness探测失败&lt;/li&gt;
&lt;li&gt;initialDelaySeconds：10指定容器启动 10之后开始执行Liveness探测，我们一般会根据应用启动的准备时间来设置。比如某个应用正常启动要花30秒，那么initialDelaySeconds的值就应该大于30&lt;/li&gt;
&lt;li&gt;periodSeconds：5指定每5秒执行一次Liveness探测。kubernetes如果连续执行3次Liveness探测均失败，则会杀掉并重启容器。&lt;/li&gt;
&lt;/ul&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/events.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/events.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;h5 id=&#34;6-3-readliness探测&#34;&gt;6.3、Readliness探测&lt;/h5&gt;

&lt;p&gt;&lt;strong&gt;用户通过Liveness探测可以告诉kubernetes什么时候通过重启容器实现自愈；readliness探测则是告诉kubernetes什么时候可以将容器加入到service负载均衡池中&lt;/strong&gt;，对外提供服务。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  labels:
    test: readiness
  name: readiness
spec:
  restartPolicy: OnFailure
  containers:
  - name: readiness
    image: busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy;sleep 30;rm -rf /tmp/healthy;sleep 600
    readinessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 10
      periodSeconds: 5
&lt;/code&gt;&lt;/pre&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/readliness.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/readliness.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;pod readiness的ready状态经历了如下变化&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;刚被创建时，ready状态为不可用&lt;/li&gt;
&lt;li&gt;15秒后（initialDelaySeconds + periodSeconds），第一次进行Readiness探测并成功返回，设置ready可用&lt;/li&gt;
&lt;li&gt;30秒后，/tmp/healthy被删除，连续3次readiness探测均失败后，ready被设置为不可用&lt;/li&gt;
&lt;/ul&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/readliness_events.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/readliness_events.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;div class=&#34;shortcode-notice note&#34;&gt;
  &lt;div class=&#34;shortcode-notice-title note&#34;&gt;
    
      比较
    
  &lt;/div&gt;
  

&lt;p&gt;&lt;strong&gt;Liveness和Readiness比较&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Liveness探测和readiness探测是两种health check机制，如果不特意配置，kubernetes将会对两种探测采取默认行为，即通过判断容器启动进程的返回值是否为0来判断探测是否成功&lt;/li&gt;
&lt;li&gt;两种探测的配置方法完全一样，支持的配置参数也一样，不同在于探测失败后的行为：Liveness探测是重启容器，readiness探测是将容器设置为不可用，不接受service转发的请求。&lt;/li&gt;
&lt;li&gt;Liveness探测和readiness探测是独立进行的，二者之间没有依赖，所以可以单独使用，也可以同时使用。用Liveness探测判断容器是否需要重启以实现自愈；用readiness探测判断容器是否已经准备好对外提供服务&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;h5 id=&#34;6-4-health-check在scale-up中的应用&#34;&gt;6.4、Health Check在Scale Up中的应用&lt;/h5&gt;

&lt;p&gt;对于多副本应用，当执行Scale Up操作时，新副本会作为backend被添加到service的负载均衡中，与已有副本一起处理客户的请求。考虑到应用启动通常都需要一个准备阶段，比如加载缓存数据、连接数据库等，从容器启动到真正能够提供服务是需要一段时间的。我们可以通过readiness探测容器是否就绪，避免将请求发送到还没准备好的backend。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  template:
    metadata:
      labels: 
        run: web
    spec:
      containers:
      - name: web
        image: myhttpd
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:
            scheme: HTTP
            path： /Healthy
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: web-svc
spec:
  selector:
    run: web
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重点关注readinessProbe部分。这里我们使用了不同于exec的另一种探测方法httpGet。kubernetes对于该方法探测成功的判断条件是http请求的返回码在200-400之间。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;schema指定协议，支持http（默认值）和https&lt;/li&gt;
&lt;li&gt;path指定访问路径&lt;/li&gt;
&lt;li&gt;port指定端口&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;配置作用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;容器启动10秒后开始探测&lt;/li&gt;
&lt;li&gt;如果http：//【container_ip】：8080/healthy返回码不是200-400，表示容器没有就绪，不接受service web-svc的请求&lt;/li&gt;
&lt;li&gt;每隔5秒探测一次&lt;/li&gt;
&lt;li&gt;直到返回码为200-400，表明容器已经就绪，然后将其添加到web-svc的负载均衡中，开始处理客户请求&lt;/li&gt;
&lt;li&gt;探测会继续以5秒的间隔执行，如果连续发生3次失败，容器又会从负载均衡中移除，直到下次探测成功重新加入&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于http：//【container_ip】：8080/healthy，应用可以实现自己的判断逻辑，比如检查所依赖的数据库是否就绪&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;定义 /healthy的处理函数&lt;/li&gt;
&lt;li&gt;连接数据库并执行测试SQL&lt;/li&gt;
&lt;li&gt;测试成功，正常返回，代码200&lt;/li&gt;
&lt;li&gt;测试失败，返回错误代码503&lt;/li&gt;
&lt;li&gt;在8080端口监听&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于生产环境中重要的应用，都建议配置Healthy Check，保证处理客户请求的容器都是准备就绪的Service backend&lt;/p&gt;

&lt;h5 id=&#34;6-5-healthy-check在滚动更新中的应用&#34;&gt;6.5、Healthy Check在滚动更新中的应用&lt;/h5&gt;

&lt;p&gt;Healthy Check另一个重要的应用场景是Rolling Update。试想一下，现有一个正常运行的多副本应用，接下来对应用进行更新（比如使用更高版本的image），kubernetes会启动新副本，发生如下事件&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;正常情况下新副本需要10秒钟完成准备工作，在此之前无法响应业务请求&lt;/li&gt;
&lt;li&gt;由于人为配置错误，副本始终无法完成准备工作（比如无法连接后端数据库）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因为新副本本身没有异常退出，默认Healthy Check机制会人为容器已经就绪，进而会逐步用新副本替换现有副本，其结果就是：当所有旧副本都被替换后，整个应用将无法处理请求，无法对外提供服务。如果这是发生在重要的生产系统上，后果非常严重。如果正确配置了Healthy Check，新副本只有通过了Readiness探测才会被添加到Service；没有通过探测，现有副本不会被全部替换，业务正常进行。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: app
spec:
  replicas: 10
  template:
    metadata:
      labels:
        run: app
    spec:
      containers:
      - name: app
        image: busybox
        args:
        - /bin/sh
        - -c
        - sleep 10; touch /tmp/healthy;sleep 30000
        readinessProbe:
          exec:
            command:
            - cat
            - /tmp/healthy
          initialDelaySeconds: 10
          periodSeconds: 5
&lt;/code&gt;&lt;/pre&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/appv1.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/appv1.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;接下来滚动更新应用，去掉 sleep 10; touch /tmp/healthy;&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/appv1R.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/appv1R.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;很显然，由于/tmp/healthy不存在，无法通过Readiness探测。先关注 kubectl get po：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;从Pod的Age栏可判断，头5个是新副本，目前处于Not Ready状态旧副本&lt;/li&gt;
&lt;li&gt;从当初10个减少到8个&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;再看 kubectl get deployment app 输出：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;ready后的10表示期望的状态是10个Ready副本&lt;/li&gt;
&lt;li&gt;Available 表示处于Ready状态的副本数，即8个旧副本&lt;/li&gt;
&lt;li&gt;up-to-date表示当前已经完成更新的副本数，即5个副本&lt;/li&gt;
&lt;li&gt;总共8个旧副本+5个新副本&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在我们的设定中，新副本始终都无法通过Readiness探测，所以这个状态会一直保持下去。&lt;/p&gt;

&lt;p&gt;我们模拟了一个滚动更新失败的场景。幸运的是：Healthy Check帮我们屏蔽了有缺陷的副本，保留了大多数的旧副本，业务没有因更新失败收到影响。&lt;/p&gt;

&lt;p&gt;为什么新创建的副本数是5个，同时只销毁了2个旧副本？原因是：滚动更新通过参数maxSurge和maxUnavailable来控制副本替换的数量。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;maxSurge&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此参数控制滚动更新过程中副本总数超过Ready上限。maxSurge可以是具体的整数（比如3），也可以是百分百，向上取整。maxSurge默认值为25%。 10 + ceil（10 * 25%） =13&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;maxUnavailable&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此参数控制滚动更新过程中，不可用的副本相占Ready的最大比例。maxUnavailable可以是具体的整数（比如3），也可以是百分百，向下取整。maxUnavailable默认值是25%。 10 - floor（10 * 25%） = 8&lt;/p&gt;

&lt;p&gt;maxSurge值越大，初创建的新副本就越多；maxUnavailable值越大，初销毁的旧副本数量就越多。理想情况下，我们这个案例滚动更新的过程为：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;创建3个新副本使副本总数达到13个；&lt;/li&gt;
&lt;li&gt;销毁2个旧副本使可用副本数降到8个&lt;/li&gt;
&lt;li&gt;当2个旧副本成功销毁时，再创建2个新副本，使副本总数保持为13个&lt;/li&gt;
&lt;li&gt;当新副本通过Readiness探测后，会使可用副本数增加，超过8&lt;/li&gt;
&lt;li&gt;进而可以继续销毁更多的旧副本，使可用副本数回到8&lt;/li&gt;
&lt;li&gt;旧副本的销毁使副本总数低于13，这样就允许创建更多的新副本&lt;/li&gt;
&lt;li&gt;这个过程会持续进行，最终所有的旧副本都会被新副本替换，滚动更新完成。&lt;/li&gt;
&lt;/ol&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/scaledup.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/scaledup.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;如果要定制maxSurge和maxUnavailable，进行配置。&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/scaleupSet.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/scaleupSet.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;h3 id=&#34;七-数据管理&#34;&gt;七、数据管理&lt;/h3&gt;

&lt;p&gt;kubernetes管理存储资源&lt;/p&gt;

&lt;h5 id=&#34;7-1-volume&#34;&gt;7.1、Volume&lt;/h5&gt;

&lt;p&gt;容器和Pod是短暂的，生命周期可能很短，会被频繁地销毁和创建。容器销毁时，保存在容器内部文件系统中的数据都会被清除。为了持久化保持容器的数据，可以使用kubernetes Volume。Volume的生命周期独立于容器，Pod中的容器可能被销毁和重建，但Volume会被保留。&lt;/p&gt;

&lt;p&gt;本质上，kubernetes Volume是一个目录，这一点与Docker Volume类似。当Volume被mount 到Pod，Pod中的所有容器都可以访问这个Volume。kubernetes Volume也支持多种backend类型，包括emptyDir、hostPath、GCE Persistent Disk、AWS Elastic Block Store、NFS，Ceph等。Volume提供了对各种backend的抽象，容器在使用Volume读写数据的时候不需要关心数据是存放到本地节点的文件系统还是云硬盘上。对它来说，所有类型的Volume都是一个目录。&lt;/p&gt;

&lt;h6 id=&#34;7-1-1-emptydir&#34;&gt;7.1.1、emptyDir&lt;/h6&gt;

&lt;p&gt;emptyDir是最基础的Volume类型。正如其名，一个emptyDir Volume是Host上的一个空目录。emptyDir Volume对于容器来说是持久的，对于Pod来说则不是。当Pod从节点删除时，Volume的内容也会被删除。但是如果容器被销毁而Pod还在，则Volume不受影响。也就是说：&lt;strong&gt;emptyDir Volume的生命周期与Pod一致。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Pod中的所有容器都可以共享Volume，它们可以指定各自的mount路径&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  name: producer-consumer
spec:
  containers:
  - image: busybox
    name: producer
    volumeMounts:
    - mountPath: /producer_dir
      name: shared-volume
    args:
    - /bin/sh
    - -c
    - echo &amp;quot;hello world&amp;quot; &amp;gt; /producer_dir/hello;sleep 3000


  - image: busybox
    name: consumer
    volumeMounts:
    - mountPath: /consumer_dir
      name: shared-volume
    args:
    - /bin/sh
    - -c
    - cat /consumer_dir/hello;sleep 3000


  volumes:
  - name: shared-volume
    emptyDir: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;模拟一个producer-consumer场景。Pod有两个容器producer和consumer，它们共享一个Volume。producer负责往Volume中写数据，consumer则从Volume读取数据。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;文件最底部volumes定义了一个emptyDir类型的Volume shared-volume&lt;/li&gt;
&lt;li&gt;producer容器将share-volume mount到/producer_dir目录&lt;/li&gt;
&lt;li&gt;producer通过echo将数据写到文件hello里&lt;/li&gt;
&lt;li&gt;consumer容器将share-volume mount到/consumer_dir目录&lt;/li&gt;
&lt;li&gt;consumer通过cat从文件hello读数据&lt;/li&gt;
&lt;/ol&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/consumer_log.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/consumer_log.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;kubectl logs显示容器consumer成功读取到producer写入的数据，验证两个容器共享emptyDir Volume。因为emptyDir是Docker Host文件系统里的目录，其效果相当于执行了docker run -v /producer_dir和docker run -v /consumer_dir。通过docker inspect查看容器的详细配置信息&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/consumer_dir.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/consumer_dir.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;




&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/producer_dir.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/producer_dir.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;这里的/var/lib/kubelet/pods/9d85a541-661e-11e9-8ef0-080027eb7f76/volumes/kubernetes.io~empty-dir/shared-volume就是emptyDir在Host上的真正路径。&lt;/p&gt;

&lt;p&gt;emptyDir是Host上创建的临时目录，其优点是能够方便地为Pod中的容器提供共享存储，不需要额外的配置。它不具有持久性，如果Pod不存在了，emptyDir也就没有了。根据这个特性，emptyDir特别适合Pod中的容器需要临时共享存储空间的场景吗，比如前面的生产者消费者用例。&lt;/p&gt;

&lt;h6 id=&#34;7-1-2-hostpath&#34;&gt;7.1.2 hostPath&lt;/h6&gt;

&lt;p&gt;hostPath Volume的作用是将Docker Host文件系统中已经存在的目录mount给Pod容器。大部分应用都不会使用hostPath Volume，因为这实际增加了Pod与节点的耦合，限制了Pod的使用。不过那些需要访问kubernetes或Docker内部数据（配置文件和二进制库）的应用则需要使用hostPath。比如kube-apiserver和kube-controller-manager就是这样的应用，通过kubectl edit &amp;ndash;namespace=kube-system pod kube-apiserver-master查看kube-apiserver Pod的配置，Volume的相关部分如下&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/hostpath.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/hostpath.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;这里定义了 三个hostPath：ca-certs、etc-pki、k8s-certs，分别对应Host目录/etc/ssl/certs、/etc/pki、/etc/kubernetes/pki&lt;/p&gt;

&lt;p&gt;如果Pod被销毁了，hostPath对应的目录还是会被保留，从这一点来看，hostPath的特性比emptyDir强。不过一旦Host崩溃了，hostPath也就无法访问了。&lt;/p&gt;

&lt;h6 id=&#34;7-1-3-外部storage-provider&#34;&gt;7.1.3、外部Storage Provider&lt;/h6&gt;

&lt;p&gt;如果kubernetes部署在诸如AWS、GCE、Azure等公有云上，可以直接使用云硬盘作为Volume。给一个AWS Elastic Block Store的例子&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s2/aws.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s2/aws.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;要在Pod中使用ESB Volume，必须先在AWS中创建，然后通过volume引用，其他云硬盘的使用方法可以参考各公有云商的官方文档。相比于emptyDir和hostPath，这些Volume类型的最大特点就是不依赖kubernetes。Volume的底层基础设施由独立的存储系统管理，与kubernetes集群是分离的。数据被持久化后，即使整个kubernetes崩溃也不会受损。&lt;/p&gt;

&lt;p&gt;当然，运维这样的存储系统通常不是一项简单的工作，特别是对可靠性、可用性和扩展性有较高要求的时候。&lt;/p&gt;

&lt;h5 id=&#34;7-2-persistentvolume-persistentvolumeclaim&#34;&gt;7.2、PersistentVolume&amp;amp;PersistentVolumeClaim&lt;/h5&gt;

&lt;p&gt;Volume提供了非常好的数据持久化方案，不过在可管理性上还有不足。拿前面的AWS EBS例子来说，要使用Volume，Pod必须事先知道如下信息：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;当前Volume来自AWS EBS&lt;/li&gt;
&lt;li&gt;EBS Volume已经提前创建，并且知道确切的volume-id。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Pod通常由应用的开发人员维护，而Volume则通常是由存储系统的管理员维护。&lt;/p&gt;

&lt;p&gt;kubernetes给出的解决方案是PersistentVolume和PersistentVolumeClaim&lt;/p&gt;

&lt;p&gt;PersistentVolume（PV）是外部存储系统中的一块存储空间，由管理员创建和维护。与Volume一样，PV 具有持久性，生命周期独立于Pod。PersistentVolumeClaim（PVC）是对PV的申请（Claim）。PVC通常由普通用户创建和维护。需要为Pod分配存储资源时，用户可以创建一个PVC，指明存储资源的容量大小和访问模式（比如只读）等信息，kubernetes会查找并提供满足条件的PV。&lt;/p&gt;

&lt;p&gt;有了PVC，用户只需要告诉kubernetes需要什么样的存储资源，而不必关心真正的空间从哪里分配、如何访问等底层信息。这些Storage Provider的底层信息交给管理员来处理，只有管理员才关心PV的细节信息。&lt;/p&gt;

&lt;p&gt;kubernetes支持多种类型的PV，比如AWS EBS、Ceph、NFS等。&lt;/p&gt;

&lt;h6 id=&#34;7-2-1-nfs-persistentvolume&#34;&gt;7.2.1、NFS PersistentVolume&lt;/h6&gt;

&lt;p&gt;作为准备工作，需要在master节点搭建一个NFS服务器。创建一个PV。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion：v1
kind: PersistentVolume
metadata:
  name: mypv1
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: nfs
  nfs:
    path: /nfsdata/pv1
    server: 192.105.56.105
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;capacity&lt;/strong&gt;指定PV的容量为1GB&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;accessModes&lt;/strong&gt;指定访问模式为ReadWriteOnce，支持的访问模式有3种：ReadWriteOnce，表示PV能以read-write模式mount到单个节点，ReadOnlyMany表示PV能以read-only模式到多个节点，ReadWriteMany表示PV能以read-write模式mount到多个节点&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PersistentVolumeReclaimPolicy&lt;/strong&gt;指定当PV的回收策略为Recycle，支持的策略有3种：Retain表示需要管理员手工回收；Recycle表示清除PV中的数据，效果相当于rm -rf /themount/*；Delete表示删除Storage Provider上的对应存储资源。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;storageClassName&lt;/strong&gt;指定PV的class为nfs。相当于为PV设置了一个分类，PVC可以指定class申请相应class的PV。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;指定PV在NFS服务器上对应的目录。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;kind: PersistentVolumeClaim
apiVersion: v1
metadata:
name: mypvc1
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
  storage: 1Gi
storageClassName: nfs
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;PVC就简单了，只需要指定PV的容量、访问模式和class。接下来就可以在Pod中使用存储了&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;kind: Pod 
apiVersion: v1
metadata: 
  name: mypod1
spec: 
  containers:
    - name: mypod1
      image: busybox
      args:
      - /bin/sh
      - -c
      - sleep 3000
  volumes:
  - name: mydata
    persistentVolumeClaim:
      claimName: mypvc1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;与普通Volume的格式类似，在volumes中PVC指定使用mypvc1申请的Volume。&lt;/p&gt;

&lt;h6 id=&#34;7-2-2-回收pv&#34;&gt;7.2.2、回收PV&lt;/h6&gt;

&lt;p&gt;当不需要使用PV时，可用删除PVC回收PV。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;kubectl delete pvc mypvc1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当PVC mypvc1 被删除后，kubernetes启动了一个新的Pod recycle-for-mypv1，这个Pod的作用相当于清除PV mypv1的数据。此时mypv1的状态为Released，表示已经解除了与mypvc1的Bound，正在清除数据，此时不可用。&lt;/p&gt;

&lt;p&gt;当数据清除完毕，mypvc1的状态重新变为Available，此时可以被新的PVC申请。/nfsdata/pv1中的文件被删除。&lt;/p&gt;

&lt;p&gt;因为PV的回收策略设置为Recycle，所以数据会被清除，但这可能不是我们想要的结果。如果我们想要保留数据，可以将策略改为Retain。&lt;/p&gt;

&lt;p&gt;PV还支持Delete的回收策略，会删除PV在Storage Provider上对应的存储空间。NFS的PV不支持Delete。&lt;/p&gt;

&lt;h6 id=&#34;8-2-3-pv动态供给&#34;&gt;8.2.3、PV动态供给&lt;/h6&gt;

&lt;p&gt;提前创建PV，然后通过PVC申请PV并在Pod使用，这是静态供给（Static Provision）。如果没有满足PVC条件的PV，会动态创建PV。相对于静态供给，动态供给具有明显的优势：不需要提前创建PV，减少了管理员的工作量，效率高。动态供给是通过StorageClass实现的，StorageClass定义了如何创建PV。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata: 
  name: standard
prosisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
reclaimPolicy: Retain
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: slow
prosisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
  zones: us-east-1d,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这两种StorageClass都会动态创建AWS EBS，不通电在于standard创建的是gp2类型的EBS，而slow创建的是io1类型的EBS。不同类型 的EBS支持的参数可参考AWS官方文档。StorageClass支持Delete和Retain两种reclaimPolicy，默认Delete与之前一样，PVC在申请PV时，只需要指定StorageClass、容量和访问模式即可&lt;/p&gt;

&lt;h5 id=&#34;7-3-数据库例子&#34;&gt;7.3、数据库例子&lt;/h5&gt;

&lt;p&gt;步骤为&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;创建PV和PVC&lt;/li&gt;
&lt;li&gt;部署Mysql&lt;/li&gt;
&lt;li&gt;向Mysql添加数据&lt;/li&gt;
&lt;li&gt;模拟节点宕机故障，kubernetes将mysql自动迁移到其他节点上&lt;/li&gt;
&lt;li&gt;验证数据一致性&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;首先创建PV和PVC&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: PersistentVolume
metadata: 
  name: mysql-pv
spec:
  accessModes:
    - ReadWriteOnce
  capaticy:
    storage: 1Gi
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs
  nfs:
    path: /nfsdata/mysql-pv
    server: 192.168.56.105
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: mysql-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: nfs
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  ports: 
  - port: 3306
  selector:
    app: mysql
---
apiVersion: apps/v1beta1
kind: Deployment
metadata: 
  name: mysql
spec:
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        apps: mysql
    spec:
      containers:
      - image: mysql: 5.6
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: password
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-persistent-storage
        persistentVolumeClaim:
          claimName: mysql-pvc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面通过客户端访问Service mysql&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -ppassword
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入到数据库，插入几条数据&lt;/p&gt;

&lt;p&gt;关闭node2节点，模拟节点宕机故障&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;shutdown now
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一段时间后，kubernetes将MySQL迁移到node1 。然后再进去，mysql服务恢复，数据完好无损。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>K8s概念详解(一)</title>
      <link>https://betterfor.github.io/post/k8s/</link>
      <pubDate>Sun, 07 Jul 2019 18:27:06 +0800</pubDate>
      
      <guid>https://betterfor.github.io/post/k8s/</guid>
      
        <description>

&lt;p&gt;​   &amp;#8195;&lt;a href=&#34;https://kubernetes.io/&#34;&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;&lt;/a&gt;是一个开源的，用于管理云平台中多个主机上的容器化的应用，Kubernetes的目标是让部署容器化的应用简单并且高效，&lt;strong&gt;Kubernetes&lt;/strong&gt;提供了应用部署，规划，更新，维护的一种机制。&lt;/p&gt;

&lt;h3 id=&#34;一-基础概念&#34;&gt;一、基础概念&lt;/h3&gt;

&lt;h5 id=&#34;1-cluster&#34;&gt;1、Cluster&lt;/h5&gt;

&lt;p&gt;&amp;#8195;Cluster是计算、存储和网络资源的集合，kubernetes利用这些资源运行各种基于容器的应用。&lt;/p&gt;

&lt;h5 id=&#34;2-master&#34;&gt;2、Master&lt;/h5&gt;

&lt;p&gt;&amp;#8195;Master是Cluster的大脑，它的主要职责是调度，即决定将应用放在哪里运行。Master运行linux操作系统，可以是物理机或虚拟机。为了实现高可用，可以运行多个Master。&lt;/p&gt;

&lt;h5 id=&#34;3-node&#34;&gt;3、Node&lt;/h5&gt;

&lt;p&gt;&amp;#8195;Node的职责试运行容器应用。Node由Master管理，Node负责监控并汇报容器的状态，同时根据Master的要求管理容器的生命周期。Node运行再Linux操作系统上，可以是物理机或虚拟机。&lt;/p&gt;

&lt;h5 id=&#34;4-pod&#34;&gt;4、Pod&lt;/h5&gt;

&lt;p&gt;&amp;#8195;Pod是kubernetes的最小工作单元。每个Pod包含一个或多个容器。Pod中的容器会作为一个整体被Master调度到一个Node上运行。Kubernetes引入Pod主要基于下面两个目的：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;可管理性有些容器天生就是需要紧密联系，一起工作。POd提供了比容器更高层次的抽象，将他们封装到一个部署单元中。kubernetes以pod为最小单位进行调度、扩展、共享资源、管理生命周期。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;通信和资源共享Pod中所有容器使用同一个网络namespace，及相同的IP地址和Port空间。它们可以直接用localhost通信。同样的，这些容器可以共享存储，当kubernetes挂在volume到pod，本质上是将volume挂在到pod中的每一个容器。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;pods有两种使用方式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;运行单一容器one-container-per-Pod是kubernetes最常见的模型，这种情况下，只是将单个容器简单封装成pod。即使是只有一个容器，kubernetes管理的也是pod而不是直接管理容器。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;运行多个容器问题在于：哪些容器应该放到一个pod中？答案是：这些容器联系必须非常紧密，而且需要直接共享资源。举个例子。pod包含两个容器：一个File Puller，一个Web Server。File Puller会定期从外部的Content Manager中拉去最新的文件，将其存放再共享的volume中，Web Server从volume读取文件，响应Consumer的请求。这两个是紧密协作的，它们一起为Consumer提供最新的数据；同时它们也通过volume共享数据，所以放到一个pod是合适的。再看一个反例：是否需要将tomcat和mysql放到一个pod中？Tomcat从Mysql读取数据，它们之间需要协作，但还不至于放到一个pod中一起部署、一起启动、一起停止。同时它们之间是通过JDBC交换数据，并不是直接共享存储，座椅放到各自的pod中更为合适。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&#34;5-controller&#34;&gt;5、Controller&lt;/h5&gt;

&lt;p&gt;&amp;#8195;kubernetes通常不会直接创建pod，而是通过Controller来管理pod。Controller中定义了pod的部署特性，比如有几个副本、再什么样的Node上运行等。为了满足不同的业务场景，kubernetes提供了多种Controller，包括Deployment、ReplicaSet、DaemonSet、StatefulSet、Job等，我们逐一讨论。&lt;/p&gt;

&lt;p&gt;（1）Deployment是最常用的Controller。可以管理pod的多个副本，并确保pod按照期望的转台运行。&lt;/p&gt;

&lt;p&gt;（2）ReplicaSet实现Pod的多副本管理。使用Department时会自动创建ReplicaSet，也就是说Department是通过ReplicaSet来管理Pod的多个副本，我们通常不需要直接使用ReplicaSet。（3）DaemonSet用于每个Node最多只运行一个pod副本的场景。正式其名所示，daemonSet通常用于daemon。&lt;/p&gt;

&lt;p&gt;（4）StatefulSet能够保证pod的每个副本再整个生命周期中名称是不变的，而其他Controller不提供这个功能。当某个Pod发生故障需要删除并重新启动时，Pod的名称会发生变化，同时StatefulSet会保证副本按照固定的顺序启动、更新或删除。&lt;/p&gt;

&lt;p&gt;（5）Job用于运行结束就删除的应用，而其他Controller中的Pod通常是长期持续运行。&lt;/p&gt;

&lt;h5 id=&#34;6-service&#34;&gt;6、Service&lt;/h5&gt;

&lt;p&gt;&amp;#8195;Deployment可以部署多个副本，每个Pod都有自己的IP，外界如何访问这些副本呢？Kubernetes Service定义了外界访问一组特定的Pod的方式。Service有自己的IP和端口，Service为Pod提供负载均衡。Kubernetes运行容器（Pod）与访问容器这两项任务分别由Controller和Service执行。&lt;/p&gt;

&lt;h5 id=&#34;7-namespace&#34;&gt;7、Namespace&lt;/h5&gt;

&lt;p&gt;&amp;#8195;如果有多个用户或项目组使用同一个Kubernetes Cluster，如何将他们创建的Controller、Pod等资源分开呢？Namespace可以将一个物理的Cluster逻辑上划分成多个虚拟Cluster，每个Cluster就是一个Namespace。不同Namespace里的资源是完全隔离的。kubernetes默认创建两个Namespace。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;default：创建资源时如果不指定，都将被放到这个Namespace中。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kube-system：kubernetes自己创建的系统资源将放到这个Namespace中。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;二-kubernetes架构&#34;&gt;二、Kubernetes架构&lt;/h3&gt;

&lt;h5 id=&#34;1-master节点&#34;&gt;1、master节点&lt;/h5&gt;

&lt;p&gt;master是kubernetes Cluster的大脑，运行着的Daemon服务包括kube-apiserver、kube-scheduler、kube-controller-manager、ectd和pod网络（例如flannel）。&lt;/p&gt;

&lt;p&gt;（1）API Server（kube-apiserver）API Server提供HTTP/HTTPS Restful API，即kubernetes APi。API Server是kubernetes Cluster的前端接口，各种客户端工具（CLI或UI）以及kubernetes其他组件可以通过它管理Cluster的各种资源。&lt;/p&gt;

&lt;p&gt;（2）Scheduler（kube-scheduler）Scheduler负责决定将Pod放在哪个Node上运行。Scheduler在调度时会充分考虑Cluster的拓扑结构，当前各个节点的负载，以及应对高可用、性能、数据亲和性的需求。&lt;/p&gt;

&lt;p&gt;（3）Controller Manager（kube-controller-manager）Controller Manager负责管理Cluster各种资源，保证资源处于预期的状态。Controller Manager由多种Controller组成，包括replication controller、endpoints controller、namespace controller、serviceaccounts controller等。不同的controller管理不同的资源。例如，replication controller管理Deployment、StatefulSet、DaemonSet的生命周期，namespace controller管理Namespace资源。&lt;/p&gt;

&lt;p&gt;（4）ectdectd负责保存kubernetes Cluster的配置信息和各种资源的状态信息。当数据发生变化时，ectd会快速地通知kubernetes相关组件。&lt;/p&gt;

&lt;p&gt;（5）Pod网络Pod要能够相互通信，kubernetes Cluster必须部署Pod网络，flannel是其中一个可选方案。&lt;/p&gt;

&lt;h5 id=&#34;2-node节点&#34;&gt;2、Node节点&lt;/h5&gt;

&lt;p&gt;Node是Pod运行的地方，kubernetes支持docker、rkt等容器Runtime。Node上运行的kubernetes组件有kubelet、kube-proxy和Pod网络。&lt;/p&gt;

&lt;p&gt;（1）kubeletkubelet是Node的agent，当Scheduler确定在某个Node上运行Pod后，会将Pod的具体配置信息（image、volume等）发送给该节点的kubelet，kubelet会根据这些信息创建和运行容器，并向Master报告运行状态。&lt;/p&gt;

&lt;p&gt;（2）kube-proxyservice在逻辑上代表了后端的多个Pod，外界通过service访问Pod。service接收到的请求是如何转发到Pod的呢？这就是kube-proxy要完成的工作。每个Node都会运行kube-proxy服务，它负责将访问service的TCP/UDP数据流转发到后端容器。如果有多个副本，kube-proxy会实现负载均衡。&lt;/p&gt;

&lt;p&gt;（3）Pod网络Pod要能够相互通信，kubernetes Cluster必须部署Pod网络，flannel是其中一个可选方案。&lt;/p&gt;

&lt;h5 id=&#34;3-部署过程&#34;&gt;3、部署过程&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;kuberctl发送部署请求到API Server&lt;/li&gt;
&lt;li&gt;API Server通知Controller Manager创建一个deployment资源&lt;/li&gt;
&lt;li&gt;Scheduler执行调度任务，将两个副本Pod分发到k8s-node1 和k8s-node2&lt;/li&gt;
&lt;li&gt;k8s-node1和k8s-node2上的kubectl在各自的节点创建并运行Pod&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;补充两点：&lt;/p&gt;

&lt;p&gt;（1）应用的配置和当前状态信息保存在ectd中，执行kubectl get pod时API Server会从ectd中读取这些数据&lt;/p&gt;

&lt;p&gt;（2）flannel会为每个Pod都分配IP。因为没有创建service，所以目前kube-proxy还没参与进来。&lt;/p&gt;

&lt;h3 id=&#34;三-运行应用&#34;&gt;三、运行应用&lt;/h3&gt;

&lt;h5 id=&#34;3-1-deployment&#34;&gt;3.1、Deployment&lt;/h5&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;kubectl run nginx-deployment --image=nginx:1.7.9 --replicas=2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的命令将部署包含2个副本的Deployment nginx-deployment，容器的image为nginx：1.7.9&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s/deploy_run.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s/deploy_run.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;




&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s/deploy_describe.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s/deploy_describe.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;主要看Events，这里告诉我们创建了一个ReplicaSet  nginx-deployment-7c7d6486fc，Events是Deployment的日志，记录了ReplicaSet的启动过程。通过上面的分析，也验证了Deployment通过ReplicaSet来管理pod的事实。&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s/rs.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s/rs.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;Controlled By指明此ReplicaSet是由Deployment nginx-deployment创建的。Events是两个副本Pod创建的日志。&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s/pods.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s/pods.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;




&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s/decribe_pod.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s/decribe_pod.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;Controlled By指明此pod是由ReplicaSet nginx-deployment-7c7d6486fc-bl8x4创建的。Events记录了Pod的启动过程。如果操作失败（比如image不存在），也能在这里找到原因。&lt;/p&gt;

&lt;p&gt;总结过程 ：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;用户通过kubectl创建Deployment&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Deployment创建ReplicaSet&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ReplicaSet创建pod&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&#34;shortcode-notice note&#34;&gt;
  &lt;div class=&#34;shortcode-notice-title note&#34;&gt;
    
      笔记
    
  &lt;/div&gt;
  

&lt;p&gt;&lt;strong&gt;命令 VS 配置文件&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;k8s通过两种创建资源方式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;用kubectl命令创建，比如“kubectl run nginx-deployment &amp;ndash;image=nginx:1.7.9 &amp;ndash;replicas=2”，在命令行中通过参数指定资源的属性&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;通过配置文件和kubectl apply创建。要完成同样的工作，可执行命令“kubectl apply -f nginx.yaml”&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
name: nginx-deployment
spec:
replicas: 2
template:
  metadata:
    labels:
      app: web_server
  spec:
    containers:
    - image: nginx:1.7.9
      name: nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;资源的属性写在配置文件中，格式为yaml&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;基于命令的方式：简单、直观、快捷、上手快；适合临时测试或实验&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;基于配置文件的方式：配置文件描述了What，即应用最终要达到的状态；配置文件提供了创建资源的模板，能够重复部署；可以像管理代码一样管理部署；适合正式的、跨环境的、规模化部署；要求熟悉配置文件语法，有一定难度&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;h6 id=&#34;3-1-1-配置文件简介&#34;&gt;3.1.1 配置文件简介&lt;/h6&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;apiVersion 是当前配置格式的版本&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kind 是要创建的资源类型&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;matadata 是该资源的元数据，name是必须的元数据项&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;spec 部分是该Deployment的规格说明&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;replicas 指明副本数量，默认为1&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;template 定义pod的模板，这里是配置文件的重要部分&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;metadata 定义pod的元数据，至少要定义一个label。label的key和value可以任意指定&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;spec 描述pod的规格，此部分定义pod中每一个容器的属性，name和image是必须的&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s/nginx_yaml.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s/nginx_yaml.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;h6 id=&#34;3-1-2-伸缩&#34;&gt;3.1.2 伸缩&lt;/h6&gt;

&lt;p&gt;新创建的副本会调度到k8s-node1和k8s-node2上。处于安全考虑，默认配置下kubernetes不会讲pod调度到Master节点上，如果希望master当做node使用&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;kubectl taint node master node-role.kubernetes.io/master-
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果恢复master only状态&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;kubectl taint node master node-role.kubernetes.io/master=&amp;quot;&amp;quot;:NoSchedule
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;3-1-3-failover&#34;&gt;3.1.3 Failover&lt;/h6&gt;

&lt;p&gt;如果节点故障，k8s会检查到节点不可用，将pod标记为Unknown，并在其他节点新建pod，维持总副本数。当节点恢复后，Unknown节点会被删除，不过已运行的pod不会重新调度回去。&lt;/p&gt;

&lt;h6 id=&#34;3-1-4-用label控制pod的位置&#34;&gt;3.1.4 用label控制pod的位置&lt;/h6&gt;

&lt;p&gt;默认配置下，Scheduler会将pod调度到所有可用的node。不过有些情况我们希望将pod部署到指定的node，比如有大量磁盘I/O的pod部署到ssd的node或gpu的node&lt;/p&gt;

&lt;p&gt;k8s通过label来实现这个功能。label是key-value对，各种资源都可以设置label，灵活添加各种自定义属性。比如执行如下命令标注node1是配置ssd的节点&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s/label.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s/label.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;disktype=ssd已经成功添加到node1，除了disktype，node还有k8s自己维护的label，有了自定义label，接下来可以指定pod部署到node1 。&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s/nginx-deploy_yaml.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s/nginx-deploy_yaml.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;在pod模板的spec里通过nodeSelector指定此pod部署到具有label disktype=ssd的node上。要删除label disktype，执行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;kubectl label node node1 disktype-
&lt;/code&gt;&lt;/pre&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s/label_node.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s/label_node.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;不过此时pod并不会重新部署，除非在nginx.yml中删除nodeSelector设置，然后通过kubectl apply重新部署，k8s会删除之前的pod并调度和运行新的pod。&lt;/p&gt;

&lt;h5 id=&#34;3-2-daemonset&#34;&gt;3.2、DaemonSet&lt;/h5&gt;

&lt;p&gt;Deployment部署的副本Pod会分布在各个Node上，每个Node都可能运行好几个副本。&lt;/p&gt;

&lt;p&gt;DaemonSet的不同之处在于：每个Node上只能运行一个副本。&lt;/p&gt;

&lt;p&gt;DaemonSet的典型应用场景：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在集群的每个节点上运行存储Daemon，比如glusterd或ceph&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在每个节点上运行日志搜集Daemon，比如flunentd或logstash&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在每个节点上运行监控Daemon，比如Prometheus Node Exporter或collectd&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其实kubernetes自己就在用DaemonSet运行系统组件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;kubectl get daemonset --namespace=kube-system
&lt;/code&gt;&lt;/pre&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s/daemonset.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s/daemonset.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;Daemo kube-flannel-ds和kube-proxy分别负责在每个节点上运行flannel和kube-proxy组件&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s/daemon_pod.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s/daemon_pod.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;因为flannel和kube-proxy属于系统组件，需要在明航通过&amp;ndash;namespace=kube-system指定namespace kube-system。若不指定，返回默认的default资源。&lt;/p&gt;

&lt;h6 id=&#34;3-2-1-kube-flannel-ds&#34;&gt;3.2.1 kube-flannel-ds&lt;/h6&gt;

&lt;p&gt;flannel的DaemonSet定义在kube-flannel.yml中。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-amd64
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/arch: amd64
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;DaemonSet配置文件的语法和结构与Deployment几乎完全一样，只是将kind设为DaemonSet。&lt;/li&gt;
&lt;li&gt;hostName指定pod直接使用node网络，相当于docker run &amp;ndash;network=host。考虑到flannel需要为集群提供网络连接，这个要求合理。&lt;/li&gt;
&lt;li&gt;containers定义了运行flannel服务的容器&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&#34;3-2-2-kube-proxy&#34;&gt;3.2.2 kube-proxy&lt;/h6&gt;

&lt;p&gt;由于无法拿到kube-proxy的yaml文件，只能运行如下命令查看配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;kubectl edit daemonset kube-proxy --namespace=kube-system
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  labels:
    k8s-app: kube-proxy
  name: kube-proxy
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: kube-proxy
  template:
    metadata:
      creationTimestamp: null
      labels:
        k8s-app: kube-proxy
    spec:
      containers:
      - command:
        - /usr/local/bin/kube-proxy
        - --config=/var/lib/kube-proxy/config.conf
        - --hostname-override=$(NODE_NAME)
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        image: k8s.gcr.io/kube-proxy:v1.14.0
        imagePullPolicy: IfNotPresent
        name: kube-proxy
        resources: {}
status:
  currentNumberScheduled: 2
  desiredNumberScheduled: 2
  numberAvailable: 2
  numberMisscheduled: 0
  numberReady: 2
  observedGeneration: 1
  updatedNumberScheduled: 2
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;kind：daemonSet指定这是一个DaemonSet类型的资源。&lt;/li&gt;
&lt;li&gt;containers定义了kube-proxy容器&lt;/li&gt;
&lt;li&gt;status是当前DaemonSet的运行时状态，这个部分是kubectl edit特有的。其实kubernetes集群中每个当前运行的资源都可以通过kubectl edit查看配置和运行状态。&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&#34;3-3-job&#34;&gt;3.3、job&lt;/h5&gt;

&lt;p&gt;容器按照持续运行时间可分为两类：服务类容器和工作类容器。&lt;/p&gt;

&lt;p&gt;服务类容器通常持续提供服务，需要一直运行，比如HTTP Server、Daemon 等。&lt;/p&gt;

&lt;p&gt;工作类容器则是一次性任务，比如批处理程序，完成后容器就退出。kubernetes的Deployment、ReplicaSet和DaemonSet都用于管理服务类容器；对于工作类容器使用Job。&lt;/p&gt;

&lt;p&gt;先看一个简单的Job配置&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: batch/v1
kind: Job
metadata:
  name: myjob
spec:
  template:
    metadata:
      name: myjob
    spec:
      containers:
      - name: hello
        image: busybox
        command: [&amp;quot;echo&amp;quot;,&amp;quot;hello k8s job&amp;quot;]
      restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;batch/v1是当前Job的APIVersion&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;指明当前资源的类型为Job&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;restartPolicy指定在什么情况下需要重启容器。对于Job，只能设置为Never或者OnFailure。对于其他controller（比如Deployment）,可以设置为Always。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s/job.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s/job.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;当启动容器失败时，根据restartPolicy：Never，此失败容器不会重启，但completions为0，所以job controller会启动新的Pod。为了终止这个行为，可以删除job。如果restartPolicy设置为OnFailure，容器失败后会自动重启。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: batch/v1
kind: Job
metadata:
  name: myjob
spec:
  parallelism: 2 # 并行数量
  completions: 6 # 完成数量
  template:
    metadata:
      name: myjob
    spec:
      containers:
      - name: hello
        image: busybox
        command: [&amp;quot;echo&amp;quot;,&amp;quot;hello k8s job&amp;quot;]
      restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每次运行2个Pod，知道共6个Pod成功完成。现实中存在需要并行处理的场景。比如批处理，每个副本（pod）都会从任务池中读取任务并执行，副本越多，执行时间越短，效率越高。&lt;/p&gt;

&lt;h5 id=&#34;3-4-定时job&#34;&gt;3.4、定时Job&lt;/h5&gt;

&lt;p&gt;linux有crom程序定时执行任务，kubernetes的CronJob提供了类似的功能，可以定时执行Job。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: batch/v2alpha1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: &amp;quot;*/1 * * * *&amp;quot;
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            command: [&amp;quot;echo&amp;quot;,&amp;quot;hello k8s job!&amp;quot;]
          restartPolicy: OnFailure
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;batch/v2alpha1是当前CronJob的APIVersion&lt;/li&gt;
&lt;li&gt;指明当前资源的类型为CronJob&lt;/li&gt;
&lt;li&gt;schedule指定什么时候运行Job，其格式与Linux cron一致。这里 */1 * * * * 的含义是每一分钟启动一次&lt;/li&gt;
&lt;li&gt;jobTemplate定义Job模板，格式与前面的job一致&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;刚启动时会失败，因为kubernetes默认没有enable CronJob功能，需要在kube-apiserver中加入这个功能。修改kube-apiserver的配置文件 /etc/kubernetes/manifests/kube-apiserver.yaml&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s/kube-apiserver_yaml.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s/kube-apiserver_yaml.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt;kube-apiserver本身也是一个pod，在启动参数加上 &amp;ndash;runtime-config=batch/v2alpha1=true即可，然后重启kubelet服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;systemctl restart kubelet.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kubectl 会重启kube-apiserver Pod。通过kubectl api-versions会确认kube-apiserver现在已经支持batch/v2alpha1&lt;/p&gt;



&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s/batch_v2alpha1.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s/batch_v2alpha1.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;




&lt;div class=&#34;box&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://betterfor.github.io/content/post/k8s/hello_job.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://betterfor.github.io/content/post/k8s/hello_job.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;


&lt;p&gt; &lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>关于</title>
      <link>https://betterfor.github.io/about/</link>
      <pubDate>Sun, 07 Jul 2019 18:19:22 +0800</pubDate>
      
      <guid>https://betterfor.github.io/about/</guid>
      
        <description>&lt;p&gt;今天是7月7日，正式开始博客之旅，欢迎star我的  &lt;a href=&#34;https://github.com/betterfor&#34;&gt;github&lt;/a&gt;&lt;/p&gt;</description>
      
    </item>
    
  </channel>
</rss>
